{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.9314\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load the Word2Vec model (this may take a few minutes)\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desert_Proving_Ground: 0.8663\n",
      "factory: 0.8367\n",
      "Ramos_Arizpe: 0.8350\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar_cosmul(positive=['plant', 'desert'], negative=['water'])\n",
    "for val in result[0:3]:\n",
    "    most_similar_key, similarity = val\n",
    "    print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 10:31:46,836 : INFO : glove-wiki-gigaword-300 downloaded\n",
      "2024-02-12 10:31:46,840 : INFO : loading projection weights from /Users/Alex/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2024-02-12 10:32:42,474 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (400000, 300) matrix of type float32 from /Users/Alex/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-02-12T10:32:42.474579', 'gensim': '4.3.2', 'python': '3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.3.1-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "# Load the Word2Vec model (this may take a few minutes)\n",
    "# glove_tw25_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "# glove_wiki50_vectors = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "glove_wiki300_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.635676383972168),\n",
       " ('king', 0.6336469650268555),\n",
       " ('monarch', 0.5814188122749329),\n",
       " ('royal', 0.543052613735199),\n",
       " ('majesty', 0.5350356698036194),\n",
       " ('throne', 0.5097099542617798),\n",
       " ('lady', 0.5045416355133057),\n",
       " ('crown', 0.49980056285858154),\n",
       " ('consort', 0.4955049455165863),\n",
       " ('mary', 0.4903523623943329)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, brown\n",
    "from nltk import FreqDist\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = glove_wiki300_vectors\n",
    "\n",
    "# Download and prepare the list of English words and Brown frequency list\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "english_words = set(words.words())\n",
    "frequency_list = FreqDist(i.lower() for i in brown.words())\n",
    "\n",
    "# Filter the model's vocabulary\n",
    "filtered_vocab = {\n",
    "    word: {\"vector\": model[word], \"frequency\": frequency_list[word]} for word in model.key_to_index\n",
    "    if word in english_words and word in frequency_list\n",
    "}\n",
    "new_kv = KeyedVectors(vector_size=model.vector_size)\n",
    "\n",
    "# Prepare lists of keys (words) and their vectors\n",
    "keys = list(filtered_vocab.keys())\n",
    "vectors = [filtered_vocab[word][\"vector\"] for word in keys]\n",
    "frequencies = [filtered_vocab[word][\"frequency\"] for word in keys]\n",
    "\n",
    "# Add all vectors in one batch\n",
    "new_kv.add_vectors(keys, vectors)\n",
    "# new_kv.save_word2vec_format('onlyWordsToVec.bin', binary=True)\n",
    "new_kv.most_similar('queen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.29509893, 'aw'), (0.2633828, 'distance'), (0.24080755, 'blues'), (0.23872589, 'arabesque'), (0.2085289, 'chlorine'), (0.14262965, 'elect'), (0.13942298, 'shipwreck'), (0.12546945, 'apple'), (0.123517394, 'rationalistic'), (0.10731378, 'undersecretary')]\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "Drop 'ne'.\n",
      "['apple', 'elephant', 'vacancy', 'psychical', 'nymphomaniac', 'republic', 'encouragingly', 'slater', 'bank', 'thudding']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordSpread = ['undertaker', 'heresy', 'kidney', 'dividend', 'moccasin', 'dramatics', 'commander', 'connect']\n",
    "def forward_selection(words, targetLength, model):\n",
    "    while len(words) < targetLength:\n",
    "        words.append(new_kv.most_similar(negative=words, topn=1)[0][0])\n",
    "    print(new_kv.rank_by_centrality(words))\n",
    "    return words\n",
    "\n",
    "def backwards_selection(words, targetLength, model):\n",
    "    while len(words) > targetLength:\n",
    "        words.remove(new_kv.rank_by_centrality(words)[0][1])\n",
    "    print(new_kv.rank_by_centrality(words))\n",
    "    return words\n",
    "\n",
    "def cosine_similarity_matrix(word_list, model):\n",
    "    \"\"\"\n",
    "    Generate a 2D array of cosine similarities between all elements in a list of words.\n",
    "\n",
    "    :param word_list: List of words to compute the cosine similarity matrix for.\n",
    "    :param model: Pre-loaded Gensim KeyedVectors model containing the word vectors.\n",
    "    :return: 2D NumPy array of cosine similarities.\n",
    "    \"\"\"\n",
    "    # Filter the list to keep only words present in the model\n",
    "    valid_words = [word for word in word_list if word in model.key_to_index]\n",
    "    \n",
    "    # Retrieve the vectors for these words\n",
    "    vectors = np.array([model[word] for word in valid_words])\n",
    "    \n",
    "    # Normalize the vectors to unit length\n",
    "    vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = np.dot(vectors_norm, vectors_norm.T)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def back_select(word_list, model):\n",
    "    \"\"\"\n",
    "    Remove the word with the highest average cosine similarity to the other elements in the list,\n",
    "    using the cosine_similarity_matrix function for computation.\n",
    "\n",
    "    :param word_list: List of words.\n",
    "    :param model: Pre-loaded Gensim KeyedVectors model containing the word vectors.\n",
    "    :return: Modified list with the word removed.\n",
    "    \"\"\"\n",
    "    # Generate cosine similarity matrix for the word list\n",
    "    sim_matrix = cosine_similarity_matrix(word_list, model)\n",
    "    \n",
    "    # Compute the average cosine similarity per word (excluding self-similarity)\n",
    "    np.fill_diagonal(sim_matrix, 0)  # Ensure self-similarity is not considered\n",
    "    avg_sim = np.mean(sim_matrix, axis=1)\n",
    "    \n",
    "    # Find the index of the word with the highest average similarity\n",
    "    max_sim_index = np.argmax(avg_sim)\n",
    "    \n",
    "    # Identify the word to remove\n",
    "    valid_words = [word for word in word_list if word in model.key_to_index]\n",
    "    word_to_remove = valid_words[max_sim_index]\n",
    "    \n",
    "    print(f\"Drop '{word_to_remove}'.\")\n",
    "    \n",
    "    # Remove the identified word from the original list and return the modified list\n",
    "    modified_list = [word for word in word_list if word != word_to_remove]\n",
    "    return modified_list\n",
    "\n",
    "def bidirectional_selection(words, targetLength, iterations, model):\n",
    "    for i in range(iterations):\n",
    "        newWord = new_kv.most_similar(negative=words, topn=1)[0][0]\n",
    "        words.append(newWord)\n",
    "        if(len(words) > targetLength):\n",
    "            words = back_select(words, new_kv)\n",
    "    return words\n",
    "\n",
    "wordSpread = [\"apple\"]\n",
    "wordSpread10 = forward_selection(wordSpread, 10, new_kv)\n",
    "\n",
    "print(bidirectional_selection([\"apple\", \"elephant\", \"vacancy\"], 10, 50, new_kv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_mean_vector',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_for_all',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(new_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: levator, Mean Distance: 1.295461893081665\n",
      "Added: woodrow, Mean Distance: 1.1436173915863037\n",
      "Added: accommodation, Mean Distance: 1.102936029434204\n",
      "Added: manto, Mean Distance: 1.0776101350784302\n",
      "Added: mobster, Mean Distance: 1.0628013610839844\n",
      "Added: gainfully, Mean Distance: 1.0435519218444824\n",
      "Added: nineteen, Mean Distance: 1.0391043424606323\n",
      "Added: bolivar, Mean Distance: 1.0277533531188965\n",
      "Added: photochemical, Mean Distance: 1.0326476097106934\n",
      "Final selection: ['apple', 'levator', 'woodrow', 'accommodation', 'manto', 'mobster', 'gainfully', 'nineteen', 'bolivar', 'photochemical']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = new_kv\n",
    "def calculate_mean_distance(word, word_list, model):\n",
    "    \"\"\"Calculate the mean distance of a word from a list of words.\"\"\"\n",
    "    if not word_list:  # Avoid division by zero\n",
    "        return float('inf')  \n",
    "    distances = model.distances(word, word_list)\n",
    "    return np.mean(distances)\n",
    "\n",
    "def forward_selection(model, initial_list, n_words, vocab_limit=10000):\n",
    "    \"\"\"\n",
    "    Perform forward selection to maximize mean distance between words in the list.\n",
    "    \n",
    "    :param model: KeyedVectors model\n",
    "    :param initial_list: list of initial words (can be empty)\n",
    "    :param n_words: target number of words in the list\n",
    "    :param vocab_limit: number of top frequent words to consider from the vocabulary\n",
    "    \"\"\"\n",
    "    selected_words = initial_list[:]\n",
    "    vocabulary = [word for word, _ in model.most_similar(negative=selected_words, topn=vocab_limit)]\n",
    "\n",
    "    while len(selected_words) < n_words and vocabulary:\n",
    "        max_distance = 0\n",
    "        best_candidate = None\n",
    "        for candidate in vocabulary:\n",
    "            mean_distance = calculate_mean_distance(candidate, selected_words, model)\n",
    "            if mean_distance > max_distance:\n",
    "                max_distance = mean_distance\n",
    "                best_candidate = candidate\n",
    "                \n",
    "        if best_candidate:\n",
    "            selected_words.append(best_candidate)\n",
    "            vocabulary.remove(best_candidate)\n",
    "            print(f\"Added: {best_candidate}, Mean Distance: {max_distance}\")\n",
    "        else:\n",
    "            break  # Stop if no suitable candidate is found\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def backward_selection(selected_words, model):\n",
    "    \"\"\"\n",
    "    Perform backward selection by removing the least distinctive word from the list\n",
    "    in a more elegant way.\n",
    "    \n",
    "    :param selected_words: The initial list of words.\n",
    "    :param model: The KeyedVectors model containing the word vectors.\n",
    "    :return: The modified list with one word removed.\n",
    "    \"\"\"\n",
    "    if len(selected_words) <= 1:\n",
    "        return selected_words  # Cannot remove words from a list with 1 or 0 elements.\n",
    "\n",
    "    # Calculate the overall mean distance for the list without each word.\n",
    "    mean_distances_without_word = [\n",
    "        (word, calculate_mean_distance(word, selected_words[:i] + selected_words[i+1:], model))\n",
    "        for i, word in enumerate(selected_words)\n",
    "    ]\n",
    "\n",
    "    # Find the word whose removal results in the highest mean distance.\n",
    "    word_to_remove, _ = min(mean_distances_without_word, key=lambda x: x[1])\n",
    "\n",
    "    # Remove the least distinctive word from the list.\n",
    "    print(f\"Removing: {word_to_remove}\")\n",
    "    selected_words.remove(word_to_remove)\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def calculate_mean_distances(word_list, model):\n",
    "    \"\"\"\n",
    "    Calculate the mean distance of each word from all other words in the list.\n",
    "    \n",
    "    :param word_list: A list of words to calculate mean distances for.\n",
    "    :param model: The KeyedVectors model containing the word vectors.\n",
    "    :return: A dictionary with words as keys and their mean distances as values.\n",
    "    \"\"\"\n",
    "    mean_distances = {}\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        if word not in model.key_to_index:\n",
    "            continue  # Skip words not in the model\n",
    "        \n",
    "        # Collect all other words except the current one\n",
    "        other_words = word_list[:i] + word_list[i+1:]\n",
    "        \n",
    "        # Filter out words not in the model\n",
    "        other_words = [w for w in other_words if w in model.key_to_index]\n",
    "        \n",
    "        if not other_words:  # If no other valid words are left, skip\n",
    "            continue\n",
    "        \n",
    "        # Calculate distances from the current word to all other words\n",
    "        distances = model.distances(word, other_words)\n",
    "        \n",
    "        # Compute the mean distance and store it\n",
    "        mean_distances[word] = np.mean(distances)\n",
    "    \n",
    "    return mean_distances\n",
    "\n",
    "# Example usage\n",
    "wordSpread = ['undertaker', 'heresy', 'kidney', 'dividend', 'moccasin', 'whetstone', 'dramatics']\n",
    "wordSpread = ['apple']\n",
    "\n",
    "selected_words = forward_selection(model, wordSpread, 10)  # Target 10 words in the list\n",
    "print(\"Final selection:\", selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
