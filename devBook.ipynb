{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.9314\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load the Word2Vec model (this may take a few minutes)\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "result = word_vectors.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desert_Proving_Ground: 0.8663\n",
      "factory: 0.8367\n",
      "Ramos_Arizpe: 0.8350\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar_cosmul(positive=['plant', 'desert'], negative=['water'])\n",
    "for val in result[0:3]:\n",
    "    most_similar_key, similarity = val\n",
    "    print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 10:31:46,836 : INFO : glove-wiki-gigaword-300 downloaded\n",
      "2024-02-12 10:31:46,840 : INFO : loading projection weights from /Users/Alex/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
      "2024-02-12 10:32:42,474 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (400000, 300) matrix of type float32 from /Users/Alex/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-02-12T10:32:42.474579', 'gensim': '4.3.2', 'python': '3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang-1500.0.40.1)]', 'platform': 'macOS-14.3.1-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "# Load the Word2Vec model (this may take a few minutes)\n",
    "# glove_tw25_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "# glove_wiki50_vectors = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "glove_wiki300_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('beefy', 0.2662779688835144),\n",
       " ('burly', 0.2654513120651245),\n",
       " ('brother', 0.26382869482040405),\n",
       " ('guy', 0.2579931318759918),\n",
       " ('himself', 0.24928799271583557),\n",
       " ('hero', 0.24625438451766968),\n",
       " ('mismeasure', 0.24512620270252228),\n",
       " ('jimmy', 0.2413175106048584),\n",
       " ('arsenal', 0.23860912024974823),\n",
       " ('mustache', 0.23555244505405426)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Filter the model's vocabulary\n",
    "filtered_vocab = {\n",
    "    word: model[word] for word in model.key_to_index\n",
    "    if word in english_words\n",
    "}\n",
    "new_kv = KeyedVectors(vector_size=model.vector_size)\n",
    "\n",
    "# Prepare lists of keys (words) and their vectors\n",
    "keys = list(filtered_vocab.keys())\n",
    "vectors = [filtered_vocab[word] for word in keys]\n",
    "new_kv.sort_by_descending_frequency()\n",
    "# Add all vectors in one batch\n",
    "new_kv.add_vectors(keys, vectors)\n",
    "new_kv.most_similar(positive='man', negative = 'woman', restrict_vocab=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.635676383972168),\n",
       " ('king', 0.6336469650268555),\n",
       " ('monarch', 0.5814188122749329),\n",
       " ('royal', 0.543052613735199),\n",
       " ('majesty', 0.5350356698036194),\n",
       " ('throne', 0.5097099542617798),\n",
       " ('lady', 0.5045416355133057),\n",
       " ('crown', 0.49980056285858154),\n",
       " ('consort', 0.4955049455165863),\n",
       " ('mary', 0.4903523623943329)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, brown\n",
    "from nltk import FreqDist\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = glove_wiki300_vectors\n",
    "\n",
    "# Download and prepare the list of English words and Brown frequency list\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "english_words = set(words.words())\n",
    "frequency_list = FreqDist(i.lower() for i in brown.words())\n",
    "\n",
    "# Filter the model's vocabulary\n",
    "filtered_vocab = {\n",
    "    word: {\"vector\": model[word], \"frequency\": frequency_list[word]} for word in model.key_to_index\n",
    "    if word in english_words and word in frequency_list\n",
    "}\n",
    "new_kv = KeyedVectors(vector_size=model.vector_size)\n",
    "\n",
    "# Prepare lists of keys (words) and their vectors\n",
    "keys = list(filtered_vocab.keys())\n",
    "vectors = [filtered_vocab[word][\"vector\"] for word in keys]\n",
    "frequencies = [filtered_vocab[word][\"frequency\"] for word in keys]\n",
    "new_kv.sort_by_descending_frequency()\n",
    "# Add all vectors in one batch\n",
    "new_kv.add_vectors(keys, vectors)\n",
    "# new_kv.save_word2vec_format('onlyWordsToVec.bin', binary=True)\n",
    "new_kv.most_similar('queen', restrict_vocab=25000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from negatives rationalistic\n",
      "from negatives undersecretary\n",
      "from negatives blues\n",
      "from negatives chlorine\n",
      "from negatives shipwreck\n",
      "from negatives elect\n",
      "from negatives arabesque\n",
      "from negatives distance\n",
      "from negatives aw\n",
      "[(0.29509893, 'aw'), (0.2633828, 'distance'), (0.24080755, 'blues'), (0.23872589, 'arabesque'), (0.2085289, 'chlorine'), (0.14262965, 'elect'), (0.13942298, 'shipwreck'), (0.12546945, 'apple'), (0.123517394, 'rationalistic'), (0.10731378, 'undersecretary')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordSpread = ['undertaker', 'heresy', 'kidney', 'dividend', 'moccasin', 'dramatics', 'commander', 'connect']\n",
    "def forward_selection(words, targetLength, model):\n",
    "    while len(words) < targetLength:\n",
    "        # opposites = [new_kv.most_similar(negative=[word], topn=1)[0][0] for word in words]\n",
    "        # print(\"from opposites\", new_kv.most_similar(positive=opposites, topn=1)[0][0])\n",
    "        print(\"from negatives\", new_kv.most_similar(negative=words, topn=1)[0][0])\n",
    "        # words.append(new_kv.most_similar(positive=opposites, topn=1)[0][0])\n",
    "        words.append(new_kv.most_similar(negative=words, topn=1)[0][0])\n",
    "    print(new_kv.rank_by_centrality(words))\n",
    "    return words\n",
    "\n",
    "def backwards_selection(words, targetLength, model):\n",
    "    while len(words) > targetLength:\n",
    "        words.remove(new_kv.rank_by_centrality(words)[0][1])\n",
    "    print(new_kv.rank_by_centrality(words))\n",
    "    return words\n",
    "\n",
    "def cosine_similarity_matrix(word_list, model):\n",
    "    \"\"\"\n",
    "    Generate a 2D array of cosine similarities between all elements in a list of words.\n",
    "\n",
    "    :param word_list: List of words to compute the cosine similarity matrix for.\n",
    "    :param model: Pre-loaded Gensim KeyedVectors model containing the word vectors.\n",
    "    :return: 2D NumPy array of cosine similarities.\n",
    "    \"\"\"\n",
    "    # Filter the list to keep only words present in the model\n",
    "    valid_words = [word for word in word_list if word in model.key_to_index]\n",
    "    \n",
    "    # Retrieve the vectors for these words\n",
    "    vectors = np.array([model[word] for word in valid_words])\n",
    "    \n",
    "    # Normalize the vectors to unit length\n",
    "    vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = np.dot(vectors_norm, vectors_norm.T)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def back_select(word_list, model):\n",
    "    \"\"\"\n",
    "    Remove the word with the highest average cosine similarity to the other elements in the list,\n",
    "    using the cosine_similarity_matrix function for computation.\n",
    "\n",
    "    :param word_list: List of words.\n",
    "    :param model: Pre-loaded Gensim KeyedVectors model containing the word vectors.\n",
    "    :return: Modified list with the word removed.\n",
    "    \"\"\"\n",
    "    # Generate cosine similarity matrix for the word list\n",
    "    sim_matrix = cosine_similarity_matrix(word_list, model)\n",
    "    \n",
    "    # Compute the average cosine similarity per word (excluding self-similarity)\n",
    "    np.fill_diagonal(sim_matrix, 0)  # Ensure self-similarity is not considered\n",
    "    avg_sim = np.mean(sim_matrix, axis=1)\n",
    "    \n",
    "    # Find the index of the word with the highest average similarity\n",
    "    max_sim_index = np.argmax(avg_sim)\n",
    "    \n",
    "    # Identify the word to remove\n",
    "    valid_words = [word for word in word_list if word in model.key_to_index]\n",
    "    word_to_remove = valid_words[max_sim_index]\n",
    "    \n",
    "    print(f\"Drop '{word_to_remove}'.\")\n",
    "    \n",
    "    # Remove the identified word from the original list and return the modified list\n",
    "    modified_list = [word for word in word_list if word != word_to_remove]\n",
    "    return modified_list\n",
    "\n",
    "def bidirectional_selection(words, targetLength, iterations, model):\n",
    "    for i in range(iterations):\n",
    "        newWord = new_kv.most_similar(negative=words, topn=1)[0][0]\n",
    "        words.append(newWord)\n",
    "        if(len(words) > targetLength):\n",
    "            words = back_select(words, new_kv)\n",
    "    return words\n",
    "\n",
    "wordSpread = [\"apple\"]\n",
    "wordSpread10 = forward_selection(wordSpread, 10, new_kv)\n",
    "\n",
    "# print(bidirectional_selection([\"apple\", \"elephant\", \"vacancy\"], 10, 50, new_kv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: levator, Mean Distance: 1.295461893081665\n",
      "Added: woodrow, Mean Distance: 1.1436173915863037\n",
      "Added: accommodation, Mean Distance: 1.102936029434204\n",
      "Added: manto, Mean Distance: 1.0776101350784302\n",
      "Added: mobster, Mean Distance: 1.0628013610839844\n",
      "Added: gainfully, Mean Distance: 1.0435519218444824\n",
      "Added: nineteen, Mean Distance: 1.0391043424606323\n",
      "Added: bolivar, Mean Distance: 1.0277533531188965\n",
      "Added: photochemical, Mean Distance: 1.0326476097106934\n",
      "Final selection: ['apple', 'levator', 'woodrow', 'accommodation', 'manto', 'mobster', 'gainfully', 'nineteen', 'bolivar', 'photochemical']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model = new_kv\n",
    "def calculate_mean_distance(word, word_list, model):\n",
    "    \"\"\"Calculate the mean distance of a word from a list of words.\"\"\"\n",
    "    if not word_list:  # Avoid division by zero\n",
    "        return float('inf')  \n",
    "    distances = model.distances(word, word_list)\n",
    "    return np.mean(distances)\n",
    "\n",
    "def forward_selection(model, initial_list, n_words, vocab_limit=10000):\n",
    "    \"\"\"\n",
    "    Perform forward selection to maximize mean distance between words in the list.\n",
    "    \n",
    "    :param model: KeyedVectors model\n",
    "    :param initial_list: list of initial words (can be empty)\n",
    "    :param n_words: target number of words in the list\n",
    "    :param vocab_limit: number of top frequent words to consider from the vocabulary\n",
    "    \"\"\"\n",
    "    selected_words = initial_list[:]\n",
    "    vocabulary = [word for word, _ in model.most_similar(negative=selected_words, topn=vocab_limit)]\n",
    "\n",
    "    while len(selected_words) < n_words and vocabulary:\n",
    "        max_distance = 0\n",
    "        best_candidate = None\n",
    "        for candidate in vocabulary:\n",
    "            mean_distance = calculate_mean_distance(candidate, selected_words, model)\n",
    "            if mean_distance > max_distance:\n",
    "                max_distance = mean_distance\n",
    "                best_candidate = candidate\n",
    "                \n",
    "        if best_candidate:\n",
    "            selected_words.append(best_candidate)\n",
    "            vocabulary.remove(best_candidate)\n",
    "            print(f\"Added: {best_candidate}, Mean Distance: {max_distance}\")\n",
    "        else:\n",
    "            break  # Stop if no suitable candidate is found\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def backward_selection(selected_words, model):\n",
    "    \"\"\"\n",
    "    Perform backward selection by removing the least distinctive word from the list\n",
    "    in a more elegant way.\n",
    "    \n",
    "    :param selected_words: The initial list of words.\n",
    "    :param model: The KeyedVectors model containing the word vectors.\n",
    "    :return: The modified list with one word removed.\n",
    "    \"\"\"\n",
    "    if len(selected_words) <= 1:\n",
    "        return selected_words  # Cannot remove words from a list with 1 or 0 elements.\n",
    "\n",
    "    # Calculate the overall mean distance for the list without each word.\n",
    "    mean_distances_without_word = [\n",
    "        (word, calculate_mean_distance(word, selected_words[:i] + selected_words[i+1:], model))\n",
    "        for i, word in enumerate(selected_words)\n",
    "    ]\n",
    "\n",
    "    # Find the word whose removal results in the highest mean distance.\n",
    "    word_to_remove, _ = min(mean_distances_without_word, key=lambda x: x[1])\n",
    "\n",
    "    # Remove the least distinctive word from the list.\n",
    "    print(f\"Removing: {word_to_remove}\")\n",
    "    selected_words.remove(word_to_remove)\n",
    "\n",
    "    return selected_words\n",
    "\n",
    "def calculate_mean_distances(word_list, model):\n",
    "    \"\"\"\n",
    "    Calculate the mean distance of each word from all other words in the list.\n",
    "    \n",
    "    :param word_list: A list of words to calculate mean distances for.\n",
    "    :param model: The KeyedVectors model containing the word vectors.\n",
    "    :return: A dictionary with words as keys and their mean distances as values.\n",
    "    \"\"\"\n",
    "    mean_distances = {}\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        if word not in model.key_to_index:\n",
    "            continue  # Skip words not in the model\n",
    "        \n",
    "        # Collect all other words except the current one\n",
    "        other_words = word_list[:i] + word_list[i+1:]\n",
    "        \n",
    "        # Filter out words not in the model\n",
    "        other_words = [w for w in other_words if w in model.key_to_index]\n",
    "        \n",
    "        if not other_words:  # If no other valid words are left, skip\n",
    "            continue\n",
    "        \n",
    "        # Calculate distances from the current word to all other words\n",
    "        distances = model.distances(word, other_words)\n",
    "        \n",
    "        # Compute the mean distance and store it\n",
    "        mean_distances[word] = np.mean(distances)\n",
    "    \n",
    "    return mean_distances\n",
    "\n",
    "# Example usage\n",
    "wordSpread = ['undertaker', 'heresy', 'kidney', 'dividend', 'moccasin', 'whetstone', 'dramatics']\n",
    "wordSpread = ['apple']\n",
    "\n",
    "selected_words = forward_selection(model, wordSpread, 10)  # Target 10 words in the list\n",
    "print(\"Final selection:\", selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 16144 words\n",
      "['blitheness', 'prolongation', 'dig', 'Myxobacteriales', 'petchary', 'flak', 'sorrow', 'vibrancy', 'belligerency', 'hoariness']\n",
      "Filtered words: ['king', 'queen', 'run', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words, wordnet\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load a broad list of English words\n",
    "all_words = set(words.words())\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Check if a word is a valid Scrabble word using WordNet to attempt to exclude proper nouns.\n",
    "    This function returns True if the word has at least one synset that is not tagged as a proper noun.\n",
    "    \"\"\"\n",
    "    # Check if the word has synsets (i.e., is recognized by WordNet)\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False  # Word is not recognized by WordNet\n",
    "    \n",
    "    # Check if the word is likely not a proper noun\n",
    "    # Note: This isn't foolproof, as it relies on WordNet's classification\n",
    "    for synset in synsets:\n",
    "        if 'noun' in synset.lexname() and not synset.name().startswith(word.lower()):\n",
    "            return True  # Word has a noun meaning that is not a proper noun\n",
    "    return False\n",
    "\n",
    "# Filter words based on the criteria\n",
    "filtered_words = {word for word in all_words if is_valid_word(word)}\n",
    "\n",
    "print(f\"Filtered {len(filtered_words)} words\")\n",
    "print(list(filtered_words)[0:10])  # Print a sample of the filtered list\n",
    "words_list = ['king', 'queen', 'Henry', 'ugh', 'run', 'jump']\n",
    "filtered_words = [word for word in words_list if is_valid_word(word)]\n",
    "\n",
    "print(\"Filtered words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Load the model (assuming it's already loaded as 'model')\n",
    "# model = gensim.downloader.load('glove-wiki-gigaword-300')  # For example\n",
    "\n",
    "# Get the vector for \"king\"\n",
    "king_vector = model.get_vector(\"king\")\n",
    "\n",
    "# Initialize a list to hold the closest words for each dimension\n",
    "closest_words = []\n",
    "\n",
    "# Iterate over each dimension in the \"king\" vector\n",
    "for dim in range(len(king_vector)):\n",
    "    min_distance = float('inf')  # Start with infinity as the minimum distance\n",
    "    closest_word = None\n",
    "    \n",
    "    # Iterate over all words in the model\n",
    "    for word in model.key_to_index.keys():\n",
    "        if word == \"king\":  # Skip \"king\" itself\n",
    "            continue\n",
    "        \n",
    "        # Calculate the absolute difference for the current dimension\n",
    "        word_vector = model.get_vector(word)\n",
    "        diff = abs(king_vector[dim] - word_vector[dim])\n",
    "        \n",
    "        # If this word is closer than the current closest, update the closest word\n",
    "        if diff < min_distance:\n",
    "            min_distance = diff\n",
    "            closest_word = word\n",
    "    \n",
    "    # Add the closest word for this dimension to the list\n",
    "    closest_words.append(closest_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sugar',\n",
       " 'fruit',\n",
       " 'corn',\n",
       " 'juice',\n",
       " 'cake',\n",
       " 'flour',\n",
       " 'garlic',\n",
       " 'pie',\n",
       " 'lemon',\n",
       " 'salad',\n",
       " 'honey',\n",
       " 'fried',\n",
       " 'shrimp',\n",
       " 'bean',\n",
       " 'vanilla',\n",
       " 'syrup',\n",
       " 'snack',\n",
       " 'paste',\n",
       " 'flavored',\n",
       " 'spicy',\n",
       " 'spinach',\n",
       " 'popcorn',\n",
       " 'almond',\n",
       " 'salmonella',\n",
       " 'cracker',\n",
       " 'mayonnaise',\n",
       " 'watermelon',\n",
       " 'fudge',\n",
       " 'tortilla',\n",
       " 'tofu',\n",
       " 'margarine',\n",
       " 'avocado',\n",
       " 'pistachio',\n",
       " 'toffee']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'model' is your word embedding model (e.g., KeyedVectors from Gensim)\n",
    "king_vector = model.get_vector(\"peanut\")\n",
    "all_vectors = model.vectors  # All word vectors in the model\n",
    "\n",
    "# Compute cosine similarities between 'king' and all other vectors\n",
    "# Normalize vectors for cosine similarity\n",
    "king_vector_norm = king_vector / np.linalg.norm(king_vector)\n",
    "all_vectors_norm = all_vectors / np.linalg.norm(all_vectors, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = np.dot(all_vectors_norm, king_vector_norm)\n",
    "\n",
    "# Specify the distance threshold Â±0.01 from a given distance (e.g., 1 for identical)\n",
    "low_threshold = .4\n",
    "high_threshold = .45\n",
    "\n",
    "# Find indices where cosine similarity is within the specified threshold, excluding the exact match\n",
    "# This step is crucial to exclude the vector for \"king\" itself, assuming it's the only perfect match\n",
    "similar_indices = np.where((cosine_similarities > low_threshold) & \n",
    "                           (cosine_similarities < high_threshold))[0]  # Exclude exact match (cosine similarity = 1)\n",
    "\n",
    "# Retrieve the corresponding words for these indices\n",
    "similar_words = [model.index_to_key[index] for index in similar_indices]\n",
    "\n",
    "similar_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lively', 'sunshine', 'colour', 'gray', 'shining', 'vivid', 'vibrant', 'purple', 'colored', 'bright']\n"
     ]
    }
   ],
   "source": [
    "def getPercentileWords(word, model):\n",
    "    cosine_similarities = model.cosine_similarities(model.get_vector(word), model.vectors)\n",
    "    sorted_indices = np.argsort(-cosine_similarities)\n",
    "    percentile_indices = np.linspace(0, len(sorted_indices) / 200, 10).astype(int)\n",
    "    percentile_words = [model.index_to_key[index] for index in sorted_indices[percentile_indices]]\n",
    "\n",
    "    return percentile_words\n",
    "\n",
    "top_n = 10000\n",
    "limited_vocab = {word: model.get_vector(word) for word in list(model.key_to_index.keys())[:top_n]}\n",
    "\n",
    "# Create a new model with this limited vocabulary\n",
    "filtered_model = KeyedVectors(vector_size=model.vector_size)\n",
    "filtered_model.add_vectors(list(limited_vocab.keys()), list(limited_vocab.values()))\n",
    "\n",
    "print(getPercentileWords(\"bright\", filtered_model)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5310387\n",
      "0.5150255\n",
      "0.43621874\n",
      "0.40101203\n",
      "0.34860992\n",
      "0.32583195\n",
      "0.26765046\n",
      "0.21883716\n",
      "['intricate', 'delicate', 'geometric', 'beautifully', 'weaving', 'maze', 'intrigue', 'devise', 'incomplete']\n"
     ]
    }
   ],
   "source": [
    "def getSpreadPercentileWords(word, model:KeyedVectors, top_n:int):\n",
    "    limited_vocab = {word: model.get_vector(word) for word in list(model.key_to_index.keys())[:top_n]}\n",
    "    # Create a new model with this limited vocabulary\n",
    "    filtered_model = KeyedVectors(vector_size=model.vector_size)\n",
    "    filtered_model.add_vectors(list(limited_vocab.keys()), list(limited_vocab.values()))\n",
    "    filtered_model.add_vector(word, model.get_vector(word))\n",
    "\n",
    "    cosine_similarities = model.cosine_similarities(filtered_model.get_vector(word), filtered_model.vectors)\n",
    "    sorted_indices = np.argsort(-cosine_similarities)\n",
    "    # percentile_indices = np.linspace(0, len(sorted_indices) / 100, 10).astype(int)\n",
    "    percentile_indices = [2**i for i in range(0, 10)]\n",
    "    percentile_words = [\"the\"]\n",
    "    chosen_word_indices = [0]\n",
    "    for a in range(1, 9):\n",
    "        binIndices = list(sorted_indices[percentile_indices[a]:percentile_indices[a+1]]) + chosen_word_indices\n",
    "        binWords = [filtered_model.index_to_key[i] for i in binIndices]\n",
    "        binDict = dict(zip(binWords, binIndices))\n",
    "        binVectors = filtered_model.vectors_for_all(binWords)\n",
    "        wordToAdd = binVectors.most_similar(negative=percentile_words)[0][0]\n",
    "        percentile_words.append(wordToAdd)\n",
    "        print(filtered_model.similarity(wordToAdd, word))\n",
    "        chosen_word_indices.append(binDict[wordToAdd])\n",
    "    percentile_words[0] = word\n",
    "\n",
    "    return percentile_words\n",
    "\n",
    "top_n = 10000\n",
    "print(getSpreadPercentileWords(\"intricate\", model, top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret word selected. Start guessing!\n",
      "Model's hint: intruding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def setup_game(model):\n",
    "    # 1. Choose a secret word randomly from the model's vocabulary\n",
    "    secret_word = random.choice(list(model.key_to_index.keys()))\n",
    "    \n",
    "    # 2. Compute cosine similarities to the secret word for all words in the model\n",
    "    secret_vector = model.get_vector(secret_word)\n",
    "    all_vectors = model.vectors  # All word vectors in the model\n",
    "    cosine_similarities = model.cosine_similarities(secret_vector, all_vectors)\n",
    "    \n",
    "    # 3. Sort indices by similarity (descending order)\n",
    "    sorted_indices = np.argsort(-cosine_similarities)\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    sorted_words = [model.index_to_key[index] for index in sorted_indices]\n",
    "    \n",
    "    return secret_word, sorted_words\n",
    "\n",
    "def model_response(model, input1, sorted_words):\n",
    "    # Ensure input1 is in the sorted words list\n",
    "    if input1 not in sorted_words:\n",
    "        return \"The word is not in the model's vocabulary.\"\n",
    "    \n",
    "    # Find the index of input1 in the sorted list\n",
    "    input1_index = sorted_words.index(input1)\n",
    "    \n",
    "    # Calculate the new index to find a word closer to the secret word\n",
    "    # but still \"above\" the user's guess in the sorted order.\n",
    "    # Ensure the calculated index is at least 0.\n",
    "    target_index = max(input1_index // 2, 0)\n",
    "    \n",
    "    # Retrieve the word at the new index\n",
    "    response_word = sorted_words[target_index]\n",
    "    \n",
    "    return response_word\n",
    "\n",
    "\n",
    "# Example usage\n",
    "secret_word, sorted_words = setup_game(model)\n",
    "print(f\"Secret word selected. Start guessing!\")\n",
    "\n",
    "# Simulating a user guess\n",
    "user_guess = \"fish\"\n",
    "response = model_response(model, user_guess, sorted_words)\n",
    "print(f\"Model's hint: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hereward', 'wandering', 'feared', 'vulnerable', 'clone', 'trooper', 'militia', 'trader', 'renegade', 'covert', 'rogue']\n"
     ]
    }
   ],
   "source": [
    "def getHalfwayCloserWords(word, model):\n",
    "    # Compute cosine similarities between the given word and all vectors in the model\n",
    "    cosine_similarities = model.cosine_similarities(model.get_vector(word), model.vectors)\n",
    "    \n",
    "    # Sort indices of vectors by their cosine similarity in descending order\n",
    "    sorted_indices = np.argsort(-cosine_similarities)\n",
    "    \n",
    "    # Initialize the list to store indices of words to be selected\n",
    "    selected_indices = []\n",
    "    current_index = 1024\n",
    "    \n",
    "    # Keep selecting words until we reach the closest word\n",
    "    while current_index > 1:\n",
    "        selected_indices.append(current_index)\n",
    "        current_index //= 2 # Move closer to the goal word\n",
    "    \n",
    "    # Adjust for 0-based indexing and ensure we include the closest word\n",
    "    adjusted_indices = [sorted_indices[i-1] for i in selected_indices if i > 0]\n",
    "    adjusted_indices.append(sorted_indices[0])  # Ensure the closest word is included\n",
    "    \n",
    "    # Retrieve the words corresponding to selected indices\n",
    "    halfway_closer_words = [model.index_to_key[index] for index in adjusted_indices]\n",
    "    \n",
    "    return halfway_closer_words\n",
    "\n",
    "# Example usage\n",
    "halfway_closer_words = getHalfwayCloserWords(\"rogue\", model)\n",
    "print(halfway_closer_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('gift', 0.4092918634414673), 'is a clue based on your guess', 'snack')\n",
      "Feedback: Far off. (Similarity: -0.02)\n",
      "(('your', 0.47820284962654114), 'is a clue based on your guess', 'gift')\n",
      "Feedback: Not very close. (Similarity: 0.30)\n",
      "(('personal', 0.5165568590164185), 'is a clue based on your guess', 'your')\n",
      "Feedback: Not very close. (Similarity: 0.36)\n",
      "(('lifelong', 0.4929089844226837), 'is a clue based on your guess', 'personal')\n",
      "Feedback: Not very close. (Similarity: 0.37)\n",
      "(('child', 0.47831782698631287), 'is a clue based on your guess', 'marriage')\n",
      "Feedback: Not very close. (Similarity: 0.33)\n",
      "(('own', 0.4845896065235138), 'is a clue based on your guess', 'family')\n",
      "Feedback: Not very close. (Similarity: 0.29)\n",
      "(('time', 0.5375105142593384), 'is a clue based on your guess', 'own')\n",
      "Feedback: Not very close. (Similarity: 0.30)\n",
      "(('work', 0.5439039468765259), 'is a clue based on your guess', 'time')\n",
      "Feedback: Not very close. (Similarity: 0.39)\n",
      "(('doing', 0.5219708681106567), 'is a clue based on your guess', 'work')\n",
      "Feedback: Not very close. (Similarity: 0.36)\n",
      "(('time', 0.5437372326850891), 'is a clue based on your guess', 'doing')\n",
      "Feedback: Not very close. (Similarity: 0.28)\n",
      "the secret word is lifetime\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import random\n",
    "top_n = 10000\n",
    "limited_vocab = {word: model.get_vector(word) for word in list(model.key_to_index.keys())[:top_n]}\n",
    "\n",
    "# Create a new model with this limited vocabulary\n",
    "filtered_model = KeyedVectors(vector_size=model.vector_size)\n",
    "filtered_model.add_vectors(list(limited_vocab.keys()), list(limited_vocab.values()))\n",
    "\n",
    "secret_word = random.choice(list(model.key_to_index.keys())[:5000])\n",
    "def find_closest_difference(target_word, guess_word, model:KeyedVectors):\n",
    "    if guess_word not in model.key_to_index or target_word not in model.key_to_index:\n",
    "        return None, \"Both words must be in the model's vocabulary.\"\n",
    "    \n",
    "    hint = model.most_similar_cosmul(positive = [target_word, guess_word])[7]\n",
    "    return hint, \"is a clue based on your guess\", guess_word\n",
    "\n",
    "# Function to provide feedback based on cosine similarity\n",
    "def provide_feedback(guess, secret_word, model):\n",
    "    guess_vec = model.get_vector(guess)\n",
    "    secret_vec = model.get_vector(secret_word)\n",
    "    cos_sim = 1 - distance.cosine(guess_vec, secret_vec)  # Cosine similarity ranges from 0 to 1\n",
    "    \n",
    "    # Categorize feedback based on similarity ranges\n",
    "    if cos_sim > 0.75:\n",
    "        feedback = \"Very close!\"\n",
    "    elif cos_sim > 0.5:\n",
    "        feedback = \"Somewhat close.\"\n",
    "    elif cos_sim > 0.25:\n",
    "        feedback = \"Not very close.\"\n",
    "    else:\n",
    "        feedback = \"Far off.\"\n",
    "    \n",
    "    return cos_sim, feedback\n",
    "\n",
    "# Game loop\n",
    "for attempt in range(10):\n",
    "    guess = input(\"Enter your guess: \").strip()\n",
    "    if guess not in model.key_to_index:\n",
    "        print(\"Word not in vocabulary. Try a different word.\")\n",
    "        continue\n",
    "    \n",
    "    similarity, feedback = provide_feedback(guess, secret_word, model)\n",
    "    print(find_closest_difference(secret_word, guess, model))\n",
    "    print(f\"Feedback: {feedback} (Similarity: {similarity:.2f})\")\n",
    "    \n",
    "    if similarity > 0.75:  # Considered a win\n",
    "        print(\"Congratulations, you've guessed closely enough!\")\n",
    "        break\n",
    "print(\"the secret word is\", secret_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
